{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebdbc2a5",
   "metadata": {},
   "source": [
    "Problem: You are given data from an audiobook app. Each customer in the database has made a purchase at least once in 2 years. We want to create a machine learning algorithm based on our data that can predict if a customer will buy again from the audiobook company.\n",
    "\n",
    "The business case action plan:\n",
    "1. Preprocess the data\n",
    "   * 1. Balance the dataset\n",
    "   * 2. Divide the dataset in training, validation and test (prevent overfitting)\n",
    "   * 3. Save the data in a tensor friendly format (.npz)\n",
    "2. Create the machine learning algorithm (same structure, different model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f5464",
   "metadata": {},
   "source": [
    "## Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d92cc9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b179a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/Mendes/Desktop/The Data Science Course 2021 - All Resources/Part_7_Deep_Learning/S51_L352/'\n",
    "raw_csv_data = np.loadtxt(path+'Audiobooks_data.csv', delimiter=',')\n",
    "#Customer ID, Book length (mins)_overall, Book length (mins)_avg, Price_overall, Price_avg, Review (yes=1, no=0), \n",
    "#Review 10/10, Minutes listened, Completion, Support Requests, Last visited minus Purchase date, \n",
    "#Targets (if this person bought another book in the following 6 months after this 2 years)\n",
    "\n",
    "unscaled_inputs_all=raw_csv_data[:,1:-1]  #the customer ID doesnÂ´t matter to us, and the target should go separately\n",
    "targets_all=raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c6155",
   "metadata": {},
   "source": [
    "## Balance the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a74a9",
   "metadata": {},
   "source": [
    "Balancing the dataset: when the number of samples are too different, the model will generate wrongly. So it's important that the data has approximatelly the same number of inputs of the different targets.\n",
    "1. We will count the number of targets that are 1s (there are less than 0s)\n",
    "2. We will keep as many 0s as 1s (we will delete the others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "610877d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets=int(np.sum(targets_all))  #count the targets that are 1s\n",
    "zero_targets_counter=0\n",
    "indices_to_remove=[]   #list to record the indices to be removed\n",
    "\n",
    "for i in range (targets_all.shape[0]):      # start of the iteration over the dataset to balance it\n",
    " if targets_all[i]==0:                      # The shape of targets_all on axis=0, is basically the length of the vector. \n",
    "  zero_targets_counter += 1                 #We want to increase the zeroes count by 1 if target is 0.\n",
    "  if zero_targets_counter>num_one_targets:  # if the target at position 'i' is 0, and the number of zeroes is bigger than the \n",
    "        #number of 1s, we want to take note of that index\n",
    "    indices_to_remove.append(i)             #adds the element to the list and I'll know the indices of all data points to be \n",
    "    #removed\n",
    "\n",
    "unscaled_inputs_equal_priors=np.delete(unscaled_inputs_all,indices_to_remove,axis=0)  #deletes from the inputs\n",
    "targets_equal_priors=np.delete(targets_all,indices_to_remove,axis=0)                  #deletes from the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8686e",
   "metadata": {},
   "source": [
    "## Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22bed811",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs=preprocessing.scale(unscaled_inputs_equal_priors) #from sklearn library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43a7d6",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a458851",
   "metadata": {},
   "source": [
    "A little trick is to shuffle the inputs and the targets. We keep the same information but in a random order because it's possible that the original dataset was collected in the order of date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1de95ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices=np.arange(scaled_inputs.shape[0]) #create a vector with the lenght of the scaled_inputs\n",
    "np.random.shuffle(shuffled_indices)                #shuffle the vector\n",
    "\n",
    "shuffled_inputs=scaled_inputs[shuffled_indices]    # create the variables with inputs and targets with the shuffled indices\n",
    "shuffled_targets=targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22e705",
   "metadata": {},
   "source": [
    "## Split the dataset into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e57b4fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800.0 3579 0.5029337803855826\n",
      "216.0 447 0.48322147651006714\n",
      "221.0 448 0.49330357142857145\n"
     ]
    }
   ],
   "source": [
    "samples_count=shuffled_inputs.shape[0]  #count the total number of samples\n",
    "\n",
    "#determine the size of samples\n",
    "train_samples_count=int(0.8*samples_count)  \n",
    "validation_samples_count=int(0.1*samples_count)\n",
    "test_samples_count=samples_count-train_samples_count-validation_samples_count\n",
    "\n",
    "#extract them\n",
    "train_inputs=shuffled_inputs[:train_samples_count]\n",
    "train_targets=shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs=shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets=shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs=shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets=shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "#it is usefull to check if we have balanced the dataset\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets)/train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets)/validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets)/test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9192ab0",
   "metadata": {},
   "source": [
    "## Save the thress datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de2972ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train',inputs=train_inputs,targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation',inputs=validation_inputs,targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test',inputs=test_inputs,targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b6dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
