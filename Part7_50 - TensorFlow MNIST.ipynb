{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c0c60aa",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69cd7d5",
   "metadata": {},
   "source": [
    "The dataset is called MNIST and refers to handwritten digit recognition. It provides 70.000 images (28x28 pixels) of handwritten digits (1 digit per image)\n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0,1,2,3,4,5,6,7,8,9), this is a classification problem with 10 classes.\n",
    "\n",
    "OUr goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf8f31a",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e6a055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mendes\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f7f95",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41370bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset,mnist_info=tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c480d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train,mnist_test=mnist_dataset['train'],mnist_dataset['test'] #this dataset doesn´t have test dataset, so we will \n",
    "#take some from the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538db7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples,tf.int64)  #cast the value of stored in the number of validation \n",
    "#samples variable to an integer, preventing any potential issues.\n",
    "\n",
    "num_test_samples=mnist_info.splits['test'].num_examples # store the number of test samples in a dedicated variable\n",
    "num_test_samples=tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e92f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'd like to scale our data in some way to make the result more numerically stable (e.g. inputs between 0 and 1). \n",
    "#it'll take an MNIST image and its label.\n",
    "def scale(image,label):\n",
    " image=tf.cast(image, tf.float32)\n",
    " image/=255.  #images contains integers between 0 and 255. if we divide for 255, floats between 0 and 1\n",
    " return image,label\n",
    "#there is a tensorflow method called 'map', which allows to apply a custom transformation to a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6b443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_and_validation_data=mnist_train.map(scale)\n",
    "test_data=mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a155d",
   "metadata": {},
   "source": [
    "shuflle the data and then create a validation dataset.\n",
    "\n",
    "shuffling = keeping the same information but in a different order\n",
    "\n",
    "If buffer_size=1, no shuffling will actually happen.\n",
    "\n",
    "If buffer_size >= num_samples, shuffling will happen at once (uniformly)\n",
    "\n",
    "If 1<buffer_size<num_samples, we will be optmizing the computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a57f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE=10000  # Used when dealing with enormous datasets, because you can´t shuffle all data at once\n",
    "shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "validation_data=shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data=shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c1d82",
   "metadata": {},
   "source": [
    "we will use mini-batch GD to train our model. It is the most efficient way to perform deep learning. For that, we must use a batch size:\n",
    "\n",
    "batch size = 1 = Stochastic gradient descent (SGD)\n",
    "\n",
    "batch size = # samples = (single batch) GD\n",
    "\n",
    "1 < batch size < # samples = mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06c52d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=100\n",
    "train_data=train_data.batch(BATCH_SIZE)#we overwrite it as there is no need to preserve a version this data that is not batched.\n",
    "# since we won´t be back propagating on the validation data, but only forward propagating, we don´t need to batch it. \n",
    "# When batching we usually find the average loss and average accuracy. During validation and testing we want the exact values. \n",
    "#Therefore, we should take all the data at once. \n",
    "\n",
    "# The model expects our validation set in batch form too, that's why we should overwrite:\n",
    "validation_data=validation_data.batch(num_validation_samples)\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs,validation_targets=next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39127052",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5e484",
   "metadata": {},
   "source": [
    "### Outline the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72abc8",
   "metadata": {},
   "source": [
    "There are 784 inputs = input layer.\n",
    "10 outputs nodes = output layer.\n",
    "We will work woth 2 hidden layers, consisting of 50 nodes each.\n",
    "I don´t know the optimal width and depth for this problem, but I surely know tese values are suboptimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1befaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=784\n",
    "output_size=10\n",
    "hidden_layer_size=100\n",
    "\n",
    "#Our data (from tfds) is such that each input is 28x28x1. We need to flat the images into vectors.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size,activation='softmax')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d8ee23",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e79196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam \n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'])#,optmizer= 'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3770e0a2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004c111",
   "metadata": {},
   "source": [
    "What happens inside an epoch\n",
    "1. At the beginning of each epoch, the training loss will be set to 0\n",
    "2. The algorithm will iterate over a preset number of batches, all from train_data\n",
    "3. The weights and biases will be updated as many times as there are batches\n",
    "4. We will get a value for the loss function, indicating how the training is going.\n",
    "5. We will also see a training accuracy\n",
    "6. At the end of the epoch, the algorithm will forward propagate the whole validation set.\n",
    "\n",
    "When we reach the maximum number of epochs the training will be over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d305204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 7s - loss: 0.3127 - accuracy: 0.9104 - val_loss: 0.1796 - val_accuracy: 0.9475 - 7s/epoch - 13ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 7s - loss: 0.1344 - accuracy: 0.9599 - val_loss: 0.1180 - val_accuracy: 0.9663 - 7s/epoch - 13ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 8s - loss: 0.0961 - accuracy: 0.9710 - val_loss: 0.1065 - val_accuracy: 0.9705 - 8s/epoch - 15ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 7s - loss: 0.0751 - accuracy: 0.9767 - val_loss: 0.0890 - val_accuracy: 0.9742 - 7s/epoch - 14ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 7s - loss: 0.0626 - accuracy: 0.9809 - val_loss: 0.0748 - val_accuracy: 0.9802 - 7s/epoch - 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11171ac8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "model.fit(train_data,epochs=NUM_EPOCHS, validation_data=(validation_inputs,validation_targets),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4967230",
   "metadata": {},
   "source": [
    "Loss should be decresing within epoch.\n",
    "\n",
    "The accuracy show in what % of the cases our outputs were equal to the targets.\n",
    "\n",
    "We usually keep an eye on the validation loss (or set early stopping mechanisms) to determine whether the model is overfitting.\n",
    "val_accuracy of the last epoch = true accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51b7ff",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ea71ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.0855 - accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy=model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69b8696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09. Test accuracy: 97.33%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss,test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd07a0",
   "metadata": {},
   "source": [
    "After we test the model, conceptually we are no longer allowed to change it.\n",
    "\n",
    "Getting a test accuracy very close to the validation accuracy shows that we have not overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827b324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
